from transformers import pipeline
from huggingface_hub import login
import streamlit as st
import torch

login(token="SEU TOKEN") # login hugging face, necessÃ¡rio token de acesso

# Defini o tÃ­tulo e o Ã­cone da pÃ¡gina
st.set_page_config(page_title="Chat PaleontolÃ³gico ğŸ¦–", page_icon="ğŸ¦•")

# Usa cache para carregar o modelo apenas uma vez
@st.cache_resource

# Cria uma pipeline de geraÃ§Ã£o de texto com o modelo Llama 3.2 3B
def carregar_modelo():
    model_id = "meta-llama/Llama-3.2-3B-Instruct"
    return pipeline(
        "text-generation",          # GeraÃ§Ã£o de texto
        model=model_id,             # Dita o modelo que serÃ¡ utilizado
        device="cuda",              # Usa GPU para acelerar a inferÃªncia, caso nÃ£o tenha GPU basta apagar essa linha
        torch_dtype=torch.float16,
        max_new_tokens=200,         # Dita qual o mÃ¡ximo de novos tokens
        do_sample=False,            # Ativa ou Desativa amostragem para gerar respostas mais variadas
        temperature=0.5,            # Esse parÃ¢metro dita o quÃ£o determinÃ­stico vai ser a resposta (valores menores sÃ£o mais determinÃ­sticos)         
        top_p=0.9,
        repetition_penalty=1.2      # Penaliza repetiÃ§Ãµes
    )

pipe = carregar_modelo()

# Cria uma chave de estado para o histÃ³rico se ainda nÃ£o existir
if "historico" not in st.session_state:
    st.session_state["historico"] = []

# TÃ­tulo e descriÃ§Ã£o da interface
st.title("ğŸ¦– Assistente em Paleontologia (LLaMA-3.2)")
st.markdown("Especialista em dinossauros e paleontologia. Pergunte Ã  vontade!")

# FormulÃ¡rio para enviar mensagem
with st.form(key="chat_form"):
    user_input = st.text_input("VocÃª:", "")  # Entrada do usuÃ¡rio
    enviar = st.form_submit_button("Enviar") # BotÃ£o para clicar e enviar

if enviar and user_input:

    # Adciona a entrada do usuÃ¡rio na lista 'histÃ³rico' em forma de dicionÃ¡rio, de modo que seja possÃ­vel identificar o autor (UsuÃ¡rio ou Assistente/Guia)
    st.session_state["historico"].append({"role": "UsuÃ¡rio", "content": user_input})

    # Inicia o Prompt vazio e adciona ao prompt a conversa salva no histÃ³rico
    prompt = ""
    for linha in st.session_state["historico"]:
        if linha['role'] == 'UsuÃ¡rio':
            prompt += f"UsuÃ¡rio: {linha['content']}\n"
        else:
            prompt += f"Assistente: {linha['content']}\n"

    # Monta o prompt completo e estruturado para o modelo e com as instruÃ§Ãµes necessÃ¡rias
    mensagem = [
        {"role": "system", "content": (
            "VocÃª Ã© um especialista em dinossauros e paleontologia. "
            "Responda apenas a Ãºltima pergunta do UsuÃ¡rio de maneira clara, direta e levando em considerando "
            "o contexto das interaÃ§Ãµes anteriores, mas caso julgue necessÃ¡rio puxe o prÃ³ximo tÃ³pico da conversa. "
            "Caso a pergunta fuja do tema paleontologia, responda: Desculpe, mas essa pergunta foge da minha especialidade."
            "NÃ£o use mais de dois parÃ¡grafos para a resposta."
        )},
        {"role": "user", "content": prompt},
    ]

    # Gera a resposta do modelo com base no prompt
    output = pipe(mensagem)
    
    # Extrai a resposta gerada 
    resposta = output[0]["generated_text"]
    resposta = resposta[-1]['content']

    # Adiciona a resposta ao histÃ³rico
    st.session_state["historico"].append({"role": "Assistente", "content": resposta})

    # Limita o tamanho do histÃ³rico para evitar prompts muito longos
    if len(st.session_state["historico"]) > 8: # Limita o histÃ³rico a 4 interaÃ§Ãµes
        st.session_state["historico"] = st.session_state["historico"][-8:]

# Mostra o histÃ³rico no formato de chat
for entrada in st.session_state["historico"]:
    if entrada["role"] == "UsuÃ¡rio":
        st.markdown(f"**ğŸ§‘ VocÃª:** {entrada['content']}")
    else:
        st.markdown(f"**ğŸ¤– Assistente:** {entrada['content']}")

